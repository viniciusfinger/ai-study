{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Aided Retrieval\n",
    "Usa a LLM para fazer a filtragem de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "chroma_directory = \"../documents/chroma_vectorstore\"\n",
    "\n",
    "metadata_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"Nome da apostila de onde o texto original foi retirado.\",\n",
    "        type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"A página da apostila de onde o texto se origina.\",\n",
    "        type=\"integer\"\n",
    "    )\n",
    "]\n",
    "\n",
    "document_description = \"Apostilas de cursos\"\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "paths = [\n",
    "    \"../documents/ApostilaLangChain.pdf\",\n",
    "    \"../documents/Explorando a API da OpenAI.pdf\",\n",
    "    \"../documents/Explorando o Universo das IAs com Hugging Face.pdf\"\n",
    "]\n",
    "\n",
    "pages = []\n",
    "\n",
    "for path in paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages.extend(loader.load())\n",
    "    \n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\",  \" \", \"\"]\n",
    ")\n",
    "\n",
    "documents = splitter.split_documents(pages)\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding,\n",
    "    persist_directory=chroma_directory\n",
    ")\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_store,\n",
    "    document_description,\n",
    "    metadata_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[retriever:Retriever > chain:query_constructor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"O que a apostila do HuggingFace fala sobre a OpenAI e o ChatGPT?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[retriever:Retriever > chain:query_constructor > prompt:FewShotPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"O que a apostila do HuggingFace fala sobre a OpenAI e o ChatGPT?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[retriever:Retriever > chain:query_constructor > prompt:FewShotPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[retriever:Retriever > chain:query_constructor > llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Your goal is to structure the user's query to match the request schema provided below.\\n\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{\\n    \\\"query\\\": string \\\\ text string to compare to document contents\\n    \\\"filter\\\": string \\\\ logical condition statement for filtering documents\\n}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` (eq | ne | gt | gte | lt | lte): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value\\n\\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` (and | or): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \\\"NO_FILTER\\\" for the filter value.\\n\\n<< Example 1. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Lyrics of a song\\\",\\n    \\\"attributes\\\": {\\n        \\\"artist\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"Name of the song artist\\\"\\n        },\\n        \\\"length\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"description\\\": \\\"Length of the song in seconds\\\"\\n        },\\n        \\\"genre\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The song genre, one of \\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"\\\"\\n        }\\n    }\\n}\\n```\\n\\nUser Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"teenager love\\\",\\n    \\\"filter\\\": \\\"and(or(eq(\\\\\\\"artist\\\\\\\", \\\\\\\"Taylor Swift\\\\\\\"), eq(\\\\\\\"artist\\\\\\\", \\\\\\\"Katy Perry\\\\\\\")), lt(\\\\\\\"length\\\\\\\", 180), eq(\\\\\\\"genre\\\\\\\", \\\\\\\"pop\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 2. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Lyrics of a song\\\",\\n    \\\"attributes\\\": {\\n        \\\"artist\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"Name of the song artist\\\"\\n        },\\n        \\\"length\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"description\\\": \\\"Length of the song in seconds\\\"\\n        },\\n        \\\"genre\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The song genre, one of \\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"\\\"\\n        }\\n    }\\n}\\n```\\n\\nUser Query:\\nWhat are songs that were not published on Spotify\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\\n\\n\\n<< Example 3. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Apostilas de cursos\\\",\\n    \\\"attributes\\\": {\\n    \\\"source\\\": {\\n        \\\"description\\\": \\\"Nome da apostila de onde o texto original foi retirado.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"page\\\": {\\n        \\\"description\\\": \\\"A p\\\\u00e1gina da apostila de onde o texto se origina.\\\",\\n        \\\"type\\\": \\\"integer\\\"\\n    }\\n}\\n}\\n```\\n\\nUser Query:\\nO que a apostila do HuggingFace fala sobre a OpenAI e o ChatGPT?\\n\\nStructured Request:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[retriever:Retriever > chain:query_constructor > llm:OpenAI] [820ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n    \\\"query\\\": \\\"HuggingFace OpenAI ChatGPT\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 869,\n",
      "      \"completion_tokens\": 28,\n",
      "      \"total_tokens\": 897\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-instruct\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[retriever:Retriever > chain:query_constructor > parser:StructuredQueryOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"```json\\n{\\n    \\\"query\\\": \\\"HuggingFace OpenAI ChatGPT\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[retriever:Retriever > chain:query_constructor > parser:StructuredQueryOutputParser] [3ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[retriever:Retriever > chain:query_constructor] [832ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "Explorando o Universo das IAs com Hugging Face\n",
      "06. Modelos de conversação e geração de texto (chatbots)\n",
      "Vamos aprender agora a usar modelos de conversação, que funcionam de forma parecida com chatbots\n",
      "como ChatGPT . Na realidade, estes modelos estão classificados na tarefa de geração de texto ( text\n",
      "generation ) no Hugging Face:\n",
      "Figure 16: Página principal da tarefa text-generation do Hugging Face.\n",
      "Vamos utilizar o modelo Felladrin/Llama-68M-Chat-v1 , disponível no link a seguir:\n",
      "==========={'id': 573, 'page': 29, 'source': 'Explorando o Universo das IAs com Hugging Face.pdf'}\n",
      "\n",
      "\n",
      "Explorando o Universo das IAs com Hugging Face\n",
      "06. Modelos de conversação e geração de texto (chatbots)\n",
      "Vamos aprender agora a usar modelos de conversação, que funcionam de forma parecida com chatbots\n",
      "como ChatGPT . Na realidade, estes modelos estão classificados na tarefa de geração de texto ( text\n",
      "generation ) no Hugging Face:\n",
      "Figure 16: Página principal da tarefa text-generation do Hugging Face.\n",
      "Vamos utilizar o modelo Felladrin/Llama-68M-Chat-v1 , disponível no link a seguir:\n",
      "==========={'page': 29, 'source': '../documents/Explorando o Universo das IAs com Hugging Face.pdf'}\n",
      "\n",
      "\n",
      "Explorando o Universo das IAs com Hugging Face\n",
      "06. Modelos de conversação e geração de texto (chatbots)\n",
      "Vamos aprender agora a usar modelos de conversação, que funcionam de forma parecida com chatbots\n",
      "como ChatGPT . Na realidade, estes modelos estão classificados na tarefa de geração de texto ( text\n",
      "generation ) no Hugging Face:\n",
      "Figure 16: Página principal da tarefa text-generation do Hugging Face.\n",
      "Vamos utilizar o modelo Felladrin/Llama-68M-Chat-v1 , disponível no link a seguir:\n",
      "==========={'page': 29, 'source': '../documents/Explorando o Universo das IAs com Hugging Face.pdf'}\n",
      "\n",
      "\n",
      "Explorando o Universo das IAs com Hugging Face\n",
      "06. Modelos de conversação e geração de texto (chatbots)\n",
      "Vamos aprender agora a usar modelos de conversação, que funcionam de forma parecida com chatbots\n",
      "como ChatGPT . Na realidade, estes modelos estão classificados na tarefa de geração de texto ( text\n",
      "generation ) no Hugging Face:\n",
      "Figure 16: Página principal da tarefa text-generation do Hugging Face.\n",
      "Vamos utilizar o modelo Felladrin/Llama-68M-Chat-v1 , disponível no link a seguir:\n",
      "==========={'page': 29, 'source': '../documents/Explorando o Universo das IAs com Hugging Face.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "question = 'O que a apostila do HuggingFace fala sobre a OpenAI e o ChatGPT?'\n",
    "\n",
    "documents_result = retriever.get_relevant_documents(question)\n",
    "\n",
    "for doc in documents_result:\n",
    "    print(doc.page_content)\n",
    "    print(f'==========={doc.metadata}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explorando o Universo das IAs com Hugging Face\n",
      "Autenticando com o token do Hugging Face\n",
      "Para usar modelos restritos, vamos precisar criar uma conta no Hugging Face e pedir por autorização\n",
      "de uso do modelo. Em seguida, precisamos criar um token de acesso (que não tem nada a ver com os\n",
      "tokens dos tokenizers ), para que nosso código entenda qual usuário está tentando usar o modelo\n",
      "e nos dê acesso a ele.\n",
      "Criando a conta\n",
      "==========={'page': 44, 'source': '../documents/Explorando o Universo das IAs com Hugging Face.pdf'}\n",
      "\n",
      "\n",
      "Explorando o Universo das IAs com Hugging Face\n",
      "Autenticando com o token do Hugging Face\n",
      "Para usar modelos restritos, vamos precisar criar uma conta no Hugging Face e pedir por autorização\n",
      "de uso do modelo. Em seguida, precisamos criar um token de acesso (que não tem nada a ver com os\n",
      "tokens dos tokenizers ), para que nosso código entenda qual usuário está tentando usar o modelo\n",
      "e nos dê acesso a ele.\n",
      "Criando a conta\n",
      "==========={'page': 44, 'source': '../documents/Explorando o Universo das IAs com Hugging Face.pdf'}\n",
      "\n",
      "\n",
      "Explorando o Universo das IAs com Hugging Face\n",
      "Autenticando com o token do Hugging Face\n",
      "Para usar modelos restritos, vamos precisar criar uma conta no Hugging Face e pedir por autorização\n",
      "de uso do modelo. Em seguida, precisamos criar um token de acesso (que não tem nada a ver com os\n",
      "tokens dos tokenizers ), para que nosso código entenda qual usuário está tentando usar o modelo\n",
      "e nos dê acesso a ele.\n",
      "Criando a conta\n",
      "==========={'page': 44, 'source': '../documents/Explorando o Universo das IAs com Hugging Face.pdf'}\n",
      "\n",
      "\n",
      "Explorando o Universo das IAs com Hugging Face\n",
      "Autenticando com o token do Hugging Face\n",
      "Para usar modelos restritos, vamos precisar criar uma conta no Hugging Face e pedir por autorização\n",
      "de uso do modelo. Em seguida, precisamos criar um token de acesso (que não tem nada a ver com os\n",
      "tokens dos tokenizers ), para que nosso código entenda qual usuário está tentando usar o modelo\n",
      "e nos dê acesso a ele.\n",
      "Criando a conta\n",
      "==========={'id': 636, 'page': 44, 'source': 'Explorando o Universo das IAs com Hugging Face.pdf'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = 'Do que se trata a página 44 da apostila de HuggingFace?'\n",
    "\n",
    "documents_result = retriever.get_relevant_documents(question)\n",
    "\n",
    "for doc in documents_result:\n",
    "    print(doc.page_content)\n",
    "    print(f'==========={doc.metadata}\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
